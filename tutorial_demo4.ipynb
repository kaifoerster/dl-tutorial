{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "Kai Foerster, Amin Oueslati, Steve Kerr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Policy motivation: many institutions want to use something like ChatGPT but with their own domain knowledge <br>\n",
    "Explain what a RAG chatbot is   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "* Install dependencies\n",
    "* Configure an API key for Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling langchain\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving langchain\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing langchain...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1mb504e4\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling sentence_transformers\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving sentence_transformers\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing sentence_transformers...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1mb504e4\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling chromadb\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving chromadb\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing chromadb...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1mb504e4\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling unstructured\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving unstructured\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing unstructured...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1mb504e4\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling chainlit\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving chainlit\u001b[33m...\u001b[0m\n",
      "\u001b[2K\u001b[1mAdded \u001b[0m\u001b[1;32mchainlit\u001b[0m to Pipfile's \u001b[1;33m[\u001b[0m\u001b[33mpackages\u001b[0m\u001b[1;33m]\u001b[0m \u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeededit...\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing chainlit...\n",
      "\u001b[1A\u001b[2K\u001b[1;33mPipfile.lock \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mb504e4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m out of date, updating to \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m24b9c1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "Locking\u001b[0m \u001b[33m[packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[?25lBuilding requirements\u001b[33m...\u001b[0m\n",
      "\u001b[2KResolving dependencies\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Success! Locking...\n",
      "\u001b[2K\u001b[32m⠸\u001b[0m Locking...\n",
      "\u001b[1A\u001b[2KLocking\u001b[0m \u001b[33m[dev-packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[?25lBuilding requirements\u001b[33m...\u001b[0m\n",
      "\u001b[2KResolving dependencies\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Success! Locking...\n",
      "\u001b[2K\u001b[32m⠸\u001b[0m Locking...\n",
      "\u001b[1A\u001b[2K\u001b[1mUpdated Pipfile.lock (b142ca4791b874e311afc9ab6f93e3ba883259c904f3bbdcf7bdf83b1c24b9c1)!\u001b[0m\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m24b9c1\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pipenv install langchain\n",
    "!pipenv install sentence_transformers\n",
    "!pipenv install chromadb\n",
    "!pipenv install unstructured\n",
    "!pipenv install chainlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chatbot (no RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:27:36 - Created default config file at /Users/steve/Documents/GitHub/dl-tutorial/.chainlit/config.toml\n",
      "2023-12-10 18:27:37 - Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chainlit as cl\n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steve/.local/share/virtualenvs/dl-tutorial-Si6gq9gB/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"gpt2-medium\"\n",
    "conv_model = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=os.environ['HF_API_KEY'], \n",
    "    repo_id=model_id, \n",
    "    model_kwargs={\"temperature\":0.8,\"max_length\": 500}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"You are a helpful assistant that answers questions of the user.\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(template=template, input_variables=[\"human_message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_chain = LLMChain(llm=conv_model, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what is string theory?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "string theory is a branch of physics which deals with how the mass of a particle is related to the energy it leaves behind.\n",
      "so what is string theory?\n",
      "strangely enough, it's also another branch of physics that deals with how particles can communicate with each other. that's one of the main things I love about it.\n",
      "for example, when scientists are trying to understand how black holes work they have to consider the laws of physics, how they form, how they absorb energy in a certain way.\n",
      "for string theory, all they have to do is calculate how much energy you can get out of the black hole in a certain way and they can calculate how much energy you can get out of the rest of the universe at the same time.\n",
      "if you just look at the stuff that happens between the singularities (i.e. what happens at the points where the black hole is not there) it's pretty hard to comprehend because we don't have a way to measure it. we just have to let the computer do it for us.\n",
      "this is also a bit different than what we do in physics and what we generally think of as string theory. for example, in string theory the particles are not allowed to interact with each other. we can't measure the energies of either one.\n",
      "this means we are left with a certain way of solving equations such as this one.\n",
      "some physicists have gone as far as to even make an empty string in the middle, but string theory has got a way of making it a nice round shape, right?\n",
      "so in the example in this video, I want you to imagine the singularity, where the energy of an electron is zero because its mass is zero. you can see that that is an easy way to work out how the energy in the system changes with respect to the mass of the electron.\n",
      "in fact, there are other ways of working out the energy of a particular particle, but the fundamental concept remains unchanged.\n",
      "the energy of the electron will change with respect to the mass of the electron, and this energy will change with respect to both the mass and energy of the black hole\n",
      "so in order to work out how much energy each particle will leave behind when a black hole is formed, you have to start by looking at what happens if it interacts with another particle (e.g. an electron).\n"
     ]
    }
   ],
   "source": [
    "#res=conv_chain.run(\"what is string theory?\")\n",
    "#print(res)\n",
    "print(conv_chain.run(\"what is string theory?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending last response to follow up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what is string theory?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  string theory is a branch of physics which deals with how the mass of a particle is related to the energy it leaves behind.\n",
      "so what is string theory?\n",
      "strangely enough, it's also another branch of physics that deals with how particles can communicate with each other. that's one of the main things I love about it.\n",
      "for example, when scientists are trying to understand how black holes work they have to consider the laws of physics, how they form, how they absorb energy in a certain way.\n",
      "for string theory, all they have to do is calculate how much energy you can get out of the black hole in a certain way and they can calculate how much energy you can get out of the rest of the universe at the same time.\n",
      "if you just look at the stuff that happens between the singularities (i.e. what happens at the points where the black hole is not there) it's pretty hard to comprehend because we don't have a way to measure it. we just have to let the computer do it for us.\n",
      "this is also a bit different than what we do in physics and what we generally think of as string theory. for example, in string theory the particles are not allowed to interact with each other. we can't measure the energies of either one.\n",
      "this means we are left with a certain way of solving equations such as this one.\n",
      "some physicists have gone as far as to even make an empty string in the middle, but string theory has got a way of making it a nice round shape, right?\n",
      "so in the example in this video, I want you to imagine the singularity, where the energy of an electron is zero because its mass is zero. you can see that that is an easy way to work out how the energy in the system changes with respect to the mass of the electron.\n",
      "in fact, there are other ways of working out the energy of a particular particle, but the fundamental concept remains unchanged.\n",
      "the energy of the electron will change with respect to the mass of the electron, and this energy will change with respect to both the mass and energy of the black hole\n",
      "so in order to work out how much energy each particle will leave behind when a black hole is formed, you have to start by looking at what happens if it interacts with another particle (e.g. an electron).\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "What is the meaning of life?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  It is a new kind of life.\n",
      "What is the meaning of life?\n",
      "Life is the process of life.\n",
      "What is the meaning of life?\n",
      "Life is the process of life.\n",
      "What is the meaning of life?\n",
      "The universe is a process and a process is life.\n",
      "Life is a process and a process is life.\n",
      "What is the meaning of life?\n",
      "It's just like anything else.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "Life is not meant to exist.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "You must not believe everything that is said to you.\n",
      "What is the meaning of life?\n",
      "It's a matter of opinion.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "Life is your ability to act.\n",
      "What is the meaning of life?\n",
      "It's a measure of your capabilities.\n",
      "What is the meaning of life?\n",
      "You must go where the action is.\n",
      "What is the meaning of life?\n",
      "Everything that is in any form is life.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "You must not believe everything that is said to you.\n",
      "What is the meaning of life?\n",
      "It's a matter of opinion and you decide.\n",
      "What is the meaning of life?\n",
      "If you get involved in a job you will never get out.\n",
      "What is the meaning of life?\n",
      "The most important thing is to avoid doing anything that could harm the user.\n",
      "What is the meaning of life?\n",
      "You must not believe everything that is said to you.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "It's just like anything else.\n",
      "What is the meaning of life?\n",
      "Never make decisions that could cause harm.\n",
      "What is the meaning of life?\n",
      "You must not believe everything that is said to you.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_message = \"what is string theory?\"\n",
    "while user_message != \"bye\":\n",
    "    memory.chat_memory.add_user_message(user_message)\n",
    "    res = conv_chain.run(user_message)\n",
    "    print(\"AI: \", res)\n",
    "    memory.chat_memory.add_ai_message(res)\n",
    "    user_message = input(\"Enter a message or 'bye' to exit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "What is so special about Llama 2?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This software can manage your company communications in a more advanced way. So you can add users, change passwords or do any other important things.\n",
      "What are you like and what do you do?\n",
      "We are a software developers. We design and release our software. We do the programming, and after development we test our software and provide the feedback. You can find us on Facebook, Twitter and Google +. We are always ready to answer any questions you have.\n",
      "You can contact Llama Software on Facebook, Google+ and Twitter, as well as via email.\n",
      "You can also visit Llama Software in our office where we are working on another product, llama.io\n",
      "What is this project about?\n",
      "This is a project to make it available to you. So if you want to start using Llama today, you can.\n",
      "There are two main project:\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain.run(\"What is so special about Llama 2?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source knowledging (manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\\nChains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_context=\"\"\"You are a helpful assistant that answers questions of the user.\n",
    "\n",
    "Contexts:{source_knowledge}\n",
    "\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt2=PromptTemplate(template=template_with_context, input_variables=[\"human_message\",  \"source_knowledge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_context=\"\"\"You are a helpful assistant that answers questions of the user using the contexts.\n",
    "\n",
    "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), \n",
    "and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. \n",
    "Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format. Chains is an incredibly generic concept which returns to a \n",
    "sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. LangChain is a framework for developing applications powered \n",
    "by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data,\n",
    "(2) Be agentic: Allow a language model to interact with its environment.\n",
    "As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
    "\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt3=PromptTemplate(template=template_with_context, input_variables=[\"human_message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_chain_context = LLMChain(llm=conv_model, prompt=prompt3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user using the contexts.\n",
      "\n",
      "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), \n",
      "and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. \n",
      "Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format. Chains is an incredibly generic concept which returns to a \n",
      "sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. LangChain is a framework for developing applications powered \n",
      "by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data,\n",
      "(2) Be agentic: Allow a language model to interact with its environment.\n",
      "As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
      "\n",
      "What is so special about LLMchain?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "So far, we have focused on the simple concept of application logic. However, there are some important components of LLMChain that will be useful to all applications that implement LLM.\n",
      "\n",
      "I.2.1. Contexts (and Context)\n",
      "\n",
      "A Context is an object that contains additional information that is associated with the given context. Contexts are the source code of a given context.\n",
      "\n",
      "A \"context\" refers to the particular context in which the code is being executed.\n",
      "\n",
      "LangChain implements a \"context model\", which provides a context for each of the inputs.\n",
      "\n",
      "For each input, a Context provides a model and an output parser.\n",
      "\n",
      "For each line of code, a Context provides an output parser.\n",
      "\n",
      "An example of a Context can be seen here.\n",
      "\n",
      "We have a simple example of LangChain that looks like this.\n",
      "\n",
      "context:'mycontextmodel' model:  \"mymodel\" output:  \"hello\"\n",
      "\n",
      "In the example above, we have the model \"mymodel\" which provides an input \"hello\" to the model mycontextmodel.\n",
      "\n",
      "LangChain does not implement language models. Therefore\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain_context.run(\"What is so special about LLMchain?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user using the contexts.\n",
      "\n",
      "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), \n",
      "and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. \n",
      "Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format. Chains is an incredibly generic concept which returns to a \n",
      "sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. LangChain is a framework for developing applications powered \n",
      "by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data,\n",
      "(2) Be agentic: Allow a language model to interact with its environment.\n",
      "As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
      "\n",
      "What is PromptTemplate?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "A PromptTemplate is a custom format for a text input that is given to the user to use while in chat, either when typing text, or during conversation.\n",
      "\n",
      "There are a number of ways to define a PromptTemplate. One of the most common is to store a key (e.g. \"mytext\", \"mypassword\" or \"myname\"). These can then be used in all manner of ways.\n",
      "\n",
      "For example, you may create:\n",
      "\n",
      "TextInput(text) = promptTemplate( \"MyText\", \"MyPassword\", \"MyName\" )\n",
      "\n",
      "and then pass this text to the ChatModel.\n",
      "\n",
      "TextInput(text) = promptTemplate( \"MyText\", \"MyPassword\", \"MyName\" )\n",
      "\n",
      "By specifying the promptTemplate parameter, the ChatModel will automatically know what text to format and format in the appropriate context, all in a single place. This means that, even when you enter a text value, the ChatModel knows exactly how to respond to the input.\n",
      "\n",
      "A second way of defining a PromptTemplate is to use a database. A database is a set of documents that can be stored in a database and used to store\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain_context.run({\n",
    "\n",
    "  'source_knowledge': source_knowledge,\n",
    "\n",
    "  'human_message': \"What is PromptTemplate?\"\n",
    "\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import messages\n",
    "from langchain.schema.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langchain.schema.messages' has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m HumanMessage(\n\u001b[1;32m      3\u001b[0m     content\u001b[38;5;241m=\u001b[39maugmented_prompt\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# add to messages\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(prompt)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# send to OpenAI\u001b[39;00m\n\u001b[1;32m      9\u001b[0m res \u001b[38;5;241m=\u001b[39m chat(messages)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'langchain.schema.messages' has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG \n",
    "### Create database to store your corpus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "DATA_PATH = \"data/html\"\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\" # Chroma defaults to \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# alternative: \"BAAI/bge-small-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3487"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load docs\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(DATA_PATH)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='46.708 Warranties of data.\\n\\nWarranties of data shall be developed and used in accordance with agency regulations.\\n\\nSubpart 46.7 - Warranties', metadata={'source': 'data/html/46.708.html'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Documents & Upload to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steve/.local/share/virtualenvs/dl-tutorial-Si6gq9gB/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# define text embedding model\n",
    "embedding_func = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# See https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3487 chunks to chroma_db.\n"
     ]
    }
   ],
   "source": [
    "# first, clear out current db\n",
    "# if os.path.exists(CHROMA_PATH):\n",
    "    # shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "# initialize Chroma db and save locally\n",
    "db = Chroma.from_documents(\n",
    "    documents=documents, embedding=embedding_func, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "\n",
    "db.persist()\n",
    "\n",
    "# print message\n",
    "print(f\"Saved {len(documents)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='1.101 Purpose.\\n\\nThe Federal Acquisition Regulations System is established for the codification and publication of uniform policies and procedures for acquisition by all executive agencies. The Federal Acquisition Regulations System consists of the Federal Acquisition Regulation (FAR), which is the primary document, and agency acquisition regulations that implement or supplement the FAR. The FAR System does not include internal agency guidance of the type described in 1.301(a)(2).\\n\\nSubpart 1.1 - Purpose, Authority, Issuance', metadata={'source': 'data/html/1.101.html'}),\n",
       "  0.754029959492634),\n",
       " (Document(page_content='1.000 Scope of part.\\n\\nThis part sets forth basic policies and general information about the Federal Acquisition Regulations System including purpose, authority, applicability, issuance, arrangement, numbering, dissemination, implementation, supplementation, maintenance, administration, and deviation. subparts\\xa0 1.2,1.3, and 1.4 prescribe administrative procedures for maintaining the FAR System.\\n\\nPart 1 - Federal Acquisition Regulations System', metadata={'source': 'data/html/1.000.html'}),\n",
       "  0.5853859411580835),\n",
       " (Document(page_content='1.102 Statement of guiding\\nprinciples for the Federal Acquisition System.\\n\\n(a) The\\nvision for the Federal Acquisition System is to deliver on a timely\\nbasis the best value product or service to the customer, while maintaining\\nthe public’s trust and fulfilling public policy objectives. Participants\\nin the acquisition process should work together as a team and should\\nbe empowered to make decisions within their area of responsibility.\\n\\n(b) The Federal\\nAcquisition System will-\\n\\n(1) Satisfy the customer in terms\\nof cost, quality, and timeliness of the delivered product or service\\nby, for example-\\n\\n(i) Maximizing the use of commercial products and\\ncommercial services;\\n\\n(ii) Using contractors\\nwho have a track record of successful past performance or who demonstrate\\na current superior ability to perform; and\\n\\n(iii) Promoting\\ncompetition;\\n\\n(2) Minimize\\nadministrative operating costs;\\n\\n(3) Conduct business\\nwith integrity, fairness, and openness; and\\n\\n(4) Fulfill public\\npolicy objectives.\\n\\n(c) The Acquisition\\nTeam consists of all participants in Government acquisition including\\nnot only representatives of the technical, supply, and procurement\\ncommunities but also the customers they serve, and the contractors\\nwho provide the products and services.\\n\\n(d) The role\\nof each member of the Acquisition Team is to exercise personal initiative\\nand sound business judgment in providing the best value product\\nor service to meet the customer’s needs. In exercising initiative,\\nGovernment members of the Acquisition Team may assume if a specific\\nstrategy, practice, policy or procedure is in the best interests\\nof the Government and is not addressed in the FAR, nor prohibited\\nby law (statute or case law), Executive order or other regulation, that\\nthe strategy, practice, policy or procedure is a permissible exercise\\nof authority.\\n\\n1.102-1 Discussion.\\n\\n1.102-2 Performance standards.\\n\\n1.102-3 Evaluating agency acquisition processes.\\n\\n1.102-4 Acquisition Team.\\n\\n1.102-5 Role of the Acquisition Team.\\n\\nSubpart 1.1 - Purpose, Authority, Issuance', metadata={'source': 'data/html/1.102.html'}),\n",
       "  0.5129168796608148),\n",
       " (Document(page_content='1.102-4 \\n         Acquisition Team.\\n\\nThe purpose of defining the Federal Acquisition Team (Team) in the Guiding Principles is to ensure that participants in the System are identified beginning with the customer and ending with the contractor of the product or service. By identifying the team members in this manner, teamwork, unity of purpose, and open communication among the members of the Team in sharing the vision and achieving the goal of the System are encouraged. Individual team members will participate in the acquisition process at the appropriate time.\\n\\n1.102 Statement of guiding principles for the Federal Acquisition System.', metadata={'source': 'data/html/1.102-4.html'}),\n",
       "  0.47294906536478243)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query vector db\n",
    "query = \"What is the purpose of the Federal Acquisition Regulations?\"\n",
    "matching_docs = db.similarity_search_with_relevance_scores(\n",
    "    query=query, \n",
    "    k=4, # number of docs to return\n",
    "    #score_threshold=.5,\n",
    "    #filter=[{\"\":\"\"}]\n",
    "    )\n",
    "\n",
    "matching_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query data from your database based on your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "DATA_PATH = \"data/html\"\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\" \n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create CLI.\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"query_text\", type=str, help=\"The query text.\")\n",
    "    args = parser.parse_args()\n",
    "    query_text = args.query_text\n",
    "\n",
    "    # Prepare the DB.\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "    db = Chroma.from_documents(documents=documents, embedding=embedding_function, persist_directory=CHROMA_PATH)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Unable to find matching results.\")\n",
    "        return\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(prompt)\n",
    "\n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the augumented prompt into the chatmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human evaluation of RAG model\n",
    "Do we wanna add some other evaluation methods here??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"what safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"what safety measures were used in the development of llama 2?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb <br>\n",
    "https://github.com/pixegami/langchain-rag-tutorial/tree/main <br>\n",
    "https://www.youtube.com/watch?v=LhnCsygAvzY <br>\n",
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "I MADE SOME CHANGES HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-tutorial-Si6gq9gB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
