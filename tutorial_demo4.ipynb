{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "Kai Foerster, Amin Oueslati, Steve Kerr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Policy motivation: many institutions want to use something like ChatGPT but with their own domain knowledge <br>\n",
    "Explain what a RAG chatbot is   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Prequsite\n",
    "\n",
    "- install libraries <br>\n",
    "- get access to OpenAI and Pinecone or Chroma for data storage"
=======
    "# Setup\n",
    "\n",
    "* Install dependencies\n",
    "* Configure an API key for Hugging Face"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU \\\n",
    "#    langchain==0.0.292 \\\n",
    "#    openai==0.28.0 \\\n",
    "#    datasets==2.10.1 \\\n",
    "#    pinecone-client==2.2.4 \\\n",
    "#    tiktoken==0.5.1"
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling langchain\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving langchain\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing langchain...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1mb504e4\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling sentence_transformers\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving sentence_transformers\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing sentence_transformers...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1mb504e4\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling chromadb\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving chromadb\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing chromadb...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1mb504e4\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling unstructured\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving unstructured\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing unstructured...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1mb504e4\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling chainlit\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving chainlit\u001b[33m...\u001b[0m\n",
      "\u001b[2K\u001b[1mAdded \u001b[0m\u001b[1;32mchainlit\u001b[0m to Pipfile's \u001b[1;33m[\u001b[0m\u001b[33mpackages\u001b[0m\u001b[1;33m]\u001b[0m \u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeededit...\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing chainlit...\n",
      "\u001b[1A\u001b[2K\u001b[1;33mPipfile.lock \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mb504e4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m out of date, updating to \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m24b9c1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "Locking\u001b[0m \u001b[33m[packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[?25lBuilding requirements\u001b[33m...\u001b[0m\n",
      "\u001b[2KResolving dependencies\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Success! Locking...\n",
      "\u001b[2K\u001b[32m⠸\u001b[0m Locking...\n",
      "\u001b[1A\u001b[2KLocking\u001b[0m \u001b[33m[dev-packages]\u001b[0m dependencies...\u001b[0m\n",
      "\u001b[?25lBuilding requirements\u001b[33m...\u001b[0m\n",
      "\u001b[2KResolving dependencies\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Success! Locking...\n",
      "\u001b[2K\u001b[32m⠸\u001b[0m Locking...\n",
      "\u001b[1A\u001b[2K\u001b[1mUpdated Pipfile.lock (b142ca4791b874e311afc9ab6f93e3ba883259c904f3bbdcf7bdf83b1c24b9c1)!\u001b[0m\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m24b9c1\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pipenv install langchain\n",
    "!pipenv install sentence_transformers\n",
    "!pipenv install chromadb\n",
    "!pipenv install unstructured\n",
    "!pipenv install chainlit"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chatbot (no RAG)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chainlit hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-10 18:27:36 - Created default config file at /Users/steve/Documents/GitHub/dl-tutorial/.chainlit/config.toml\n",
      "2023-12-10 18:27:37 - Loaded .env file\n"
     ]
    }
   ],
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "source": [
    "import os\n",
    "import chainlit as cl\n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_API_TOKEN = \"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\"\n",
    "os.environ[\"HF_API_TOKEN\"] = HF_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
=======
   "execution_count": 5,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "c:\\Users\\kaius\\.virtualenvs\\dl-tutorial-jB9oBwoe\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\kaius\\.virtualenvs\\dl-tutorial-jB9oBwoe\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
=======
      "/Users/steve/.local/share/virtualenvs/dl-tutorial-Si6gq9gB/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"gpt2-medium\"\n",
<<<<<<< HEAD
    "conv_model = HuggingFaceHub(huggingfacehub_api_token=os.environ['HF_API_TOKEN'],repo_id=model_id, model_kwargs={\"temperature\":0.8,\"max_length\": 500})"
=======
    "conv_model = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=os.environ['HF_API_KEY'], \n",
    "    repo_id=model_id, \n",
    "    model_kwargs={\"temperature\":0.8,\"max_length\": 500}\n",
    "    )"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 6,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"You are a helpful assistant that answers questions of the user.\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(template=template, input_variables=[\"human_message\"])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 7,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_chain = LLMChain(llm=conv_model, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 8,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what is string theory?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
<<<<<<< HEAD
      "string theory proposes that there are many possible universes, each of which has an infinite number of possible paths to the ending point.\n",
      "string theory is a logical theory that proposes that all of the objects in the universe are, in essence, sets of all possible combinations of them.\n",
      "This is a very interesting idea, and one that has been tried before. In this paper, the authors will try to explain why string theory is valid, and why string theory provides a very good explanation of some of the mathematical problems we face as a civilization.\n",
      "I'm going to write in simple terms - there are 3 dimensions to string theory. They are the string, the theory and the space - in string theory, a string is a set of all possible combinations of strings. For example, in the above picture of a box, there is a string in the middle. The theory describes what the box is made of - for example, it describes that the box is made of wood, which describes how the wood is arranged around the box. For further reasons, the theory that describes the box is called \"the string theory\".\n",
      "The theory is a set of strings. It is the set that corresponds to the set of all the possible combinations of strings as a set of all possible combinations of strings. There is a theory in string theory that corresponds to the space. The space is a space, such as a square, which is composed of all the spaces in a given area of a given space, such as a rectangular area.\n",
      "Since the set of all possible combinations of strings has the same number of dimensions, the theory has all the same properties as the space.\n",
      "For example, it has the property that we can find any string in the set of strings by examining it. In our example, the string is a string of four numbers. The theory tells us that any string in our box, which is a four-number string, is a four-number string. The theory also tells us that any string of any number, that is a four-number string, is a four-number string. Thus, the theory has properties similar to the space.\n",
      "What could be the difference between the theory and the space?\n",
      "The difference was discovered because the theory tells us that the set of all possible combinations of strings corresponds to the set of all possible combinations of any string as a set of all possible\n"
=======
      "string theory is a branch of physics which deals with how the mass of a particle is related to the energy it leaves behind.\n",
      "so what is string theory?\n",
      "strangely enough, it's also another branch of physics that deals with how particles can communicate with each other. that's one of the main things I love about it.\n",
      "for example, when scientists are trying to understand how black holes work they have to consider the laws of physics, how they form, how they absorb energy in a certain way.\n",
      "for string theory, all they have to do is calculate how much energy you can get out of the black hole in a certain way and they can calculate how much energy you can get out of the rest of the universe at the same time.\n",
      "if you just look at the stuff that happens between the singularities (i.e. what happens at the points where the black hole is not there) it's pretty hard to comprehend because we don't have a way to measure it. we just have to let the computer do it for us.\n",
      "this is also a bit different than what we do in physics and what we generally think of as string theory. for example, in string theory the particles are not allowed to interact with each other. we can't measure the energies of either one.\n",
      "this means we are left with a certain way of solving equations such as this one.\n",
      "some physicists have gone as far as to even make an empty string in the middle, but string theory has got a way of making it a nice round shape, right?\n",
      "so in the example in this video, I want you to imagine the singularity, where the energy of an electron is zero because its mass is zero. you can see that that is an easy way to work out how the energy in the system changes with respect to the mass of the electron.\n",
      "in fact, there are other ways of working out the energy of a particular particle, but the fundamental concept remains unchanged.\n",
      "the energy of the electron will change with respect to the mass of the electron, and this energy will change with respect to both the mass and energy of the black hole\n",
      "so in order to work out how much energy each particle will leave behind when a black hole is formed, you have to start by looking at what happens if it interacts with another particle (e.g. an electron).\n"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
     ]
    }
   ],
   "source": [
    "#res=conv_chain.run(\"what is string theory?\")\n",
    "#print(res)\n",
    "print(conv_chain.run(\"what is string theory?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending last response to follow up question"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 9,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 10,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"history\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 12,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what is string theory?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
<<<<<<< HEAD
      "AI:  string theory is a branch of mathematics which combines the laws of physics with the mathematical foundations of mathematics. string theory was originally proposed by the French physicist, Pierre-Simon Laplace in 1892. His theory is to explain why the Universe works in this way. The first formal proof of string theory came from the work of mathematician Roger Penrose in 1970, published in 1968. This was a mathematical proof of the existence of a special mathematical sub-string, which is the property of the string of space-time. It is possible to prove that the Universe does in fact exist. Another major theory in string theory is quantum mechanics. The theory states that the universe is made up of particles with the same properties of their classical counterparts. There may be some particles which have similar properties, which have not been noticed by scientists.\n",
      "what is a string?\n",
      "A string is an actual chain of individual electrons or\n",
=======
      "AI:  string theory is a branch of physics which deals with how the mass of a particle is related to the energy it leaves behind.\n",
      "so what is string theory?\n",
      "strangely enough, it's also another branch of physics that deals with how particles can communicate with each other. that's one of the main things I love about it.\n",
      "for example, when scientists are trying to understand how black holes work they have to consider the laws of physics, how they form, how they absorb energy in a certain way.\n",
      "for string theory, all they have to do is calculate how much energy you can get out of the black hole in a certain way and they can calculate how much energy you can get out of the rest of the universe at the same time.\n",
      "if you just look at the stuff that happens between the singularities (i.e. what happens at the points where the black hole is not there) it's pretty hard to comprehend because we don't have a way to measure it. we just have to let the computer do it for us.\n",
      "this is also a bit different than what we do in physics and what we generally think of as string theory. for example, in string theory the particles are not allowed to interact with each other. we can't measure the energies of either one.\n",
      "this means we are left with a certain way of solving equations such as this one.\n",
      "some physicists have gone as far as to even make an empty string in the middle, but string theory has got a way of making it a nice round shape, right?\n",
      "so in the example in this video, I want you to imagine the singularity, where the energy of an electron is zero because its mass is zero. you can see that that is an easy way to work out how the energy in the system changes with respect to the mass of the electron.\n",
      "in fact, there are other ways of working out the energy of a particular particle, but the fundamental concept remains unchanged.\n",
      "the energy of the electron will change with respect to the mass of the electron, and this energy will change with respect to both the mass and energy of the black hole\n",
      "so in order to work out how much energy each particle will leave behind when a black hole is formed, you have to start by looking at what happens if it interacts with another particle (e.g. an electron).\n",
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
<<<<<<< HEAD
      "continue your sentence\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  As long as you answer only basic questions, you will surely reach a higher level. The more you understand, the more you will expand your knowledge. You will become a professional person, and your name will be known. The more you learn, the more you will become an expert and you will be able to meet the demands of your clients. For example, in our company, we have a certain number of employees who are very smart, and they often take advantage of our services. We have to ask them to put in a couple of hours each day to help us, and they need to do that for us, but we have no idea what they know; so we have to ask them to go up and take the exam. They have to get a lot out of it, too. So they have to be productive. How can they not? One thing that we learn from these people is that there is\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what branch of mathematics does it belong to?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  There are many answers to that question. One of the ways in which mathematicians have answered it is by using the symbols for the first two branches.\n",
      "the following symbols for the first two branches are:\n",
      "a = a\n",
      "g = g\n",
      "n = n+2\n",
      "A = a+1\n",
      "b = b+1\n",
      "a-b = a-b+1\n",
      "c = c+1\n",
      "d = d+1\n",
      "There are many other answers to that question. One of the ways in which mathematicians have answered it is by using the symbols for the first two branches.\n",
      "the following symbols for the first two branches are: The following symbols for the first two branches are:\n",
      "the following symbols for the first two branches are:\n",
      "the following symbols for the first two branches are:\n",
      "The following symbols for the first two branches are:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what is string theory?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  string theory is a branch of mathematics which combines the laws of physics with the mathematical foundations of mathematics. string theory was originally proposed by the French physicist, Pierre-Simon Laplace in 1892. His theory is to explain why the Universe works in this way. The first formal proof of string theory came from the work of mathematician Roger Penrose in 1970, published in 1968. This was a mathematical proof of the existence of a special mathematical sub-string, which is the property of the string of space-time. It is possible to prove that the Universe does in fact exist. Another major theory in string theory is quantum mechanics. The theory states that the universe is made up of particles with the same properties of their classical counterparts. There may be some particles which have similar properties, which have not been noticed by scientists.\n",
      "what is a string?\n",
      "A string is an actual chain of individual electrons or\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what laws of physics does it combine it with?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  \"All natural phenomena\" is a phrase that is very common in physics textbooks. The thing is that all natural phenomena are just that: phenomena. They do not need to be analyzed or defined. All natural phenomena are a way of describing the world. In physics, the question is when and how matter and energy are combined with each other. The answer is that it is as simple as that. What is the simplest way to explain it? That is the question.\n",
      "What makes it simpler is that, for example, that it is not always the case that all phenomena are similar and similar are different and so on. Sometimes the phenomena are only related to the same thing or to another thing. For example, a rock floating in a river may not float in the same way. You see, in physics it is the same matter as another rock. In other words, a rock\n"
=======
      "What is the meaning of life?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  It is a new kind of life.\n",
      "What is the meaning of life?\n",
      "Life is the process of life.\n",
      "What is the meaning of life?\n",
      "Life is the process of life.\n",
      "What is the meaning of life?\n",
      "The universe is a process and a process is life.\n",
      "Life is a process and a process is life.\n",
      "What is the meaning of life?\n",
      "It's just like anything else.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "Life is not meant to exist.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "You must not believe everything that is said to you.\n",
      "What is the meaning of life?\n",
      "It's a matter of opinion.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "Life is your ability to act.\n",
      "What is the meaning of life?\n",
      "It's a measure of your capabilities.\n",
      "What is the meaning of life?\n",
      "You must go where the action is.\n",
      "What is the meaning of life?\n",
      "Everything that is in any form is life.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "You must not believe everything that is said to you.\n",
      "What is the meaning of life?\n",
      "It's a matter of opinion and you decide.\n",
      "What is the meaning of life?\n",
      "If you get involved in a job you will never get out.\n",
      "What is the meaning of life?\n",
      "The most important thing is to avoid doing anything that could harm the user.\n",
      "What is the meaning of life?\n",
      "You must not believe everything that is said to you.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "It's just like anything else.\n",
      "What is the meaning of life?\n",
      "Never make decisions that could cause harm.\n",
      "What is the meaning of life?\n",
      "You must not believe everything that is said to you.\n",
      "What is the meaning of life?\n",
      "Nothing is what it seems.\n",
      "What is the meaning of life?\n",
      "\n"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
     ]
    }
   ],
   "source": [
    "user_message = \"what is string theory?\"\n",
    "while user_message != \"bye\":\n",
    "    memory.chat_memory.add_user_message(user_message)\n",
    "    res = conv_chain.run(user_message)\n",
<<<<<<< HEAD
    "    print(\"AI: \",res)\n",
    "    memory.chat_memory.add_ai_message(res)\n",
    "    user_message = input(\"Enter a message or bye to exit!\")"
=======
    "    print(\"AI: \", res)\n",
    "    memory.chat_memory.add_ai_message(res)\n",
    "    user_message = input(\"Enter a message or 'bye' to exit!\")"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucinations"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": null,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
<<<<<<< HEAD
      "What is LLMChain?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The LLMChain platform is based on Blockchain.info (BOI). The main purpose of BOI is to facilitate peer-to-peer transactions between groups of nodes and also from nodes to the network (P2P) for managing and managing the distributed assets.\n",
      "This project is developing the blockchain technology to create a platform for the creation of distributed applications and networks, such as peer-to-peer lending, real-time auctions, peer-to-peer payments, and more which can be used to build, manage and share economic, social and environmental solutions.\n",
      "The BOI model is based on a distributed transaction ledger that is replicated across the network while transactions are recorded on a block-wide shared public ledger. The blockchain data is secured by a distributed consensus mechanism.\n",
      "The BOI platform provides the foundation for a peer-to-peer lending platform along with other features of a blockchain like decentralized governance and peer-to-peer ownership of assets on a decentralized platform which offers incentives for innovation, development and use.\n",
      "What is the BOI Project?\n",
      "The BOI project is the infrastructure of the project, which is the foundation of the blockchain platform. It is a platform for sharing and managing the economic, social and environmental assets of the world. It is intended that BOI's will be used in the world to build, manage and share solutions to all types of problems.\n",
      "How will this platform be used?\n",
      "This platform will work by creating a shared public ledger on which the BOI can store, secure and distribute economic, social and environmental assets.\n",
      "What will be the benefits of the BOI platform?\n",
      "The BOI platform will be able to enable faster and cheaper access to loans, real estate, land, and other assets. By creating a shared public ledger, the BOI will have access to loans, real estate, land and other assets directly to lend, purchase, exchange and transfer them. The BOI platform will also offer a common set of rules and guidelines for the use of the assets.\n",
      "What do I need to do?\n",
      "To create or use this platform you will need to register with BOI, and verify the identity of the people you wish to use it with and that they are in your name.\n",
      "The BOI project will provide a set of rules and guidelines that can guide how people can access assets such as land\n"
=======
      "What is so special about Llama 2?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This software can manage your company communications in a more advanced way. So you can add users, change passwords or do any other important things.\n",
      "What are you like and what do you do?\n",
      "We are a software developers. We design and release our software. We do the programming, and after development we test our software and provide the feedback. You can find us on Facebook, Twitter and Google +. We are always ready to answer any questions you have.\n",
      "You can contact Llama Software on Facebook, Google+ and Twitter, as well as via email.\n",
      "You can also visit Llama Software in our office where we are working on another product, llama.io\n",
      "What is this project about?\n",
      "This is a project to make it available to you. So if you want to start using Llama today, you can.\n",
      "There are two main project:\n"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(conv_chain.run(\"What is LLMChain?\"))"
=======
    "print(conv_chain.run(\"What is so special about Llama 2?\"))"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source knowledging (manual)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 13,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\\nChains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_context=\"\"\"You are a helpful assistant that answers questions of the user.\n",
    "\n",
    "Contexts:{source_knowledge}\n",
    "\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt2=PromptTemplate(template=template_with_context, input_variables=[\"human_message\",  \"source_knowledge\"])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_context=\"\"\"You are a helpful assistant that answers questions of the user using the contexts.\n",
    "\n",
    "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), \n",
    "and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. \n",
    "Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format. Chains is an incredibly generic concept which returns to a \n",
    "sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. LangChain is a framework for developing applications powered \n",
    "by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data,\n",
    "(2) Be agentic: Allow a language model to interact with its environment.\n",
    "As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
    "\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt3=PromptTemplate(template=template_with_context, input_variables=[\"human_message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_chain_context = LLMChain(llm=conv_model, prompt=prompt3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "You are a helpful assistant that answers questions of the user.\n",
      "\n",
      "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\n",
      "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
      "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
      "\n",
      "What is a LLMChain?\n",
      "\n"
=======
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user using the contexts.\n",
      "\n",
      "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), \n",
      "and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. \n",
      "Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format. Chains is an incredibly generic concept which returns to a \n",
      "sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. LangChain is a framework for developing applications powered \n",
      "by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data,\n",
      "(2) Be agentic: Allow a language model to interact with its environment.\n",
      "As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
      "\n",
      "What is so special about LLMchain?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "So far, we have focused on the simple concept of application logic. However, there are some important components of LLMChain that will be useful to all applications that implement LLM.\n",
      "\n",
      "I.2.1. Contexts (and Context)\n",
      "\n",
      "A Context is an object that contains additional information that is associated with the given context. Contexts are the source code of a given context.\n",
      "\n",
      "A \"context\" refers to the particular context in which the code is being executed.\n",
      "\n",
      "LangChain implements a \"context model\", which provides a context for each of the inputs.\n",
      "\n",
      "For each input, a Context provides a model and an output parser.\n",
      "\n",
      "For each line of code, a Context provides an output parser.\n",
      "\n",
      "An example of a Context can be seen here.\n",
      "\n",
      "We have a simple example of LangChain that looks like this.\n",
      "\n",
      "context:'mycontextmodel' model:  \"mymodel\" output:  \"hello\"\n",
      "\n",
      "In the example above, we have the model \"mymodel\" which provides an input \"hello\" to the model mycontextmodel.\n",
      "\n",
      "LangChain does not implement language models. Therefore\n"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(prompt2.format(human_message=\"What is a LLMChain?\", source_knowledge=source_knowledge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_chain = LLMChain(llm=conv_model, prompt=prompt2, verbose=True)"
=======
    "print(conv_chain_context.run(\"What is so special about LLMchain?\"))"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
<<<<<<< HEAD
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "\n",
      "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\n",
      "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
      "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
      "\n",
      "What is LLMChain?\n",
=======
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user using the contexts.\n",
      "\n",
      "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), \n",
      "and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. \n",
      "Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format. Chains is an incredibly generic concept which returns to a \n",
      "sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. LangChain is a framework for developing applications powered \n",
      "by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data,\n",
      "(2) Be agentic: Allow a language model to interact with its environment.\n",
      "As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
      "\n",
      "What is PromptTemplate?\n",
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
<<<<<<< HEAD
      "LLMChain is a framework for creating and building LLM chains, where each chain has its own model, a library, a code generator, and a code interpreter.\n"
=======
      "A PromptTemplate is a custom format for a text input that is given to the user to use while in chat, either when typing text, or during conversation.\n",
      "\n",
      "There are a number of ways to define a PromptTemplate. One of the most common is to store a key (e.g. \"mytext\", \"mypassword\" or \"myname\"). These can then be used in all manner of ways.\n",
      "\n",
      "For example, you may create:\n",
      "\n",
      "TextInput(text) = promptTemplate( \"MyText\", \"MyPassword\", \"MyName\" )\n",
      "\n",
      "and then pass this text to the ChatModel.\n",
      "\n",
      "TextInput(text) = promptTemplate( \"MyText\", \"MyPassword\", \"MyName\" )\n",
      "\n",
      "By specifying the promptTemplate parameter, the ChatModel will automatically know what text to format and format in the appropriate context, all in a single place. This means that, even when you enter a text value, the ChatModel knows exactly how to respond to the input.\n",
      "\n",
      "A second way of defining a PromptTemplate is to use a database. A database is a set of documents that can be stored in a database and used to store\n"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(context_chain.run({\n",
    "\n",
    "  'source_knowledge': source_knowledge,\n",
    "\n",
    "  'human_message': \"What is LLMChain?\"\n",
=======
    "print(conv_chain_context.run({\n",
    "\n",
    "  'source_knowledge': source_knowledge,\n",
    "\n",
    "  'human_message': \"What is PromptTemplate?\"\n",
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
    "\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 20,
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import messages\n",
    "from langchain.schema.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langchain.schema.messages' has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m HumanMessage(\n\u001b[1;32m      3\u001b[0m     content\u001b[38;5;241m=\u001b[39maugmented_prompt\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# add to messages\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(prompt)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# send to OpenAI\u001b[39;00m\n\u001b[1;32m      9\u001b[0m res \u001b[38;5;241m=\u001b[39m chat(messages)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'langchain.schema.messages' has no attribute 'append'"
     ]
    }
   ],
   "source": [
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG \n",
    "### Create database to store your corpus on"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/books\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    generate_data_store()\n",
    "\n",
    "\n",
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "DATA_PATH = \"data/html\"\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\" # Chroma defaults to \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# alternative: \"BAAI/bge-small-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3487"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load docs\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(DATA_PATH)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='46.708 Warranties of data.\\n\\nWarranties of data shall be developed and used in accordance with agency regulations.\\n\\nSubpart 46.7 - Warranties', metadata={'source': 'data/html/46.708.html'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Documents & Upload to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steve/.local/share/virtualenvs/dl-tutorial-Si6gq9gB/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# define text embedding model\n",
    "embedding_func = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# See https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3487 chunks to chroma_db.\n"
     ]
    }
   ],
   "source": [
    "# first, clear out current db\n",
    "# if os.path.exists(CHROMA_PATH):\n",
    "    # shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "# initialize Chroma db and save locally\n",
    "db = Chroma.from_documents(\n",
    "    documents=documents, embedding=embedding_func, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "\n",
    "db.persist()\n",
    "\n",
    "# print message\n",
    "print(f\"Saved {len(documents)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='1.101 Purpose.\\n\\nThe Federal Acquisition Regulations System is established for the codification and publication of uniform policies and procedures for acquisition by all executive agencies. The Federal Acquisition Regulations System consists of the Federal Acquisition Regulation (FAR), which is the primary document, and agency acquisition regulations that implement or supplement the FAR. The FAR System does not include internal agency guidance of the type described in 1.301(a)(2).\\n\\nSubpart 1.1 - Purpose, Authority, Issuance', metadata={'source': 'data/html/1.101.html'}),\n",
       "  0.754029959492634),\n",
       " (Document(page_content='1.000 Scope of part.\\n\\nThis part sets forth basic policies and general information about the Federal Acquisition Regulations System including purpose, authority, applicability, issuance, arrangement, numbering, dissemination, implementation, supplementation, maintenance, administration, and deviation. subparts\\xa0 1.2,1.3, and 1.4 prescribe administrative procedures for maintaining the FAR System.\\n\\nPart 1 - Federal Acquisition Regulations System', metadata={'source': 'data/html/1.000.html'}),\n",
       "  0.5853859411580835),\n",
       " (Document(page_content='1.102 Statement of guiding\\nprinciples for the Federal Acquisition System.\\n\\n(a) The\\nvision for the Federal Acquisition System is to deliver on a timely\\nbasis the best value product or service to the customer, while maintaining\\nthe public’s trust and fulfilling public policy objectives. Participants\\nin the acquisition process should work together as a team and should\\nbe empowered to make decisions within their area of responsibility.\\n\\n(b) The Federal\\nAcquisition System will-\\n\\n(1) Satisfy the customer in terms\\nof cost, quality, and timeliness of the delivered product or service\\nby, for example-\\n\\n(i) Maximizing the use of commercial products and\\ncommercial services;\\n\\n(ii) Using contractors\\nwho have a track record of successful past performance or who demonstrate\\na current superior ability to perform; and\\n\\n(iii) Promoting\\ncompetition;\\n\\n(2) Minimize\\nadministrative operating costs;\\n\\n(3) Conduct business\\nwith integrity, fairness, and openness; and\\n\\n(4) Fulfill public\\npolicy objectives.\\n\\n(c) The Acquisition\\nTeam consists of all participants in Government acquisition including\\nnot only representatives of the technical, supply, and procurement\\ncommunities but also the customers they serve, and the contractors\\nwho provide the products and services.\\n\\n(d) The role\\nof each member of the Acquisition Team is to exercise personal initiative\\nand sound business judgment in providing the best value product\\nor service to meet the customer’s needs. In exercising initiative,\\nGovernment members of the Acquisition Team may assume if a specific\\nstrategy, practice, policy or procedure is in the best interests\\nof the Government and is not addressed in the FAR, nor prohibited\\nby law (statute or case law), Executive order or other regulation, that\\nthe strategy, practice, policy or procedure is a permissible exercise\\nof authority.\\n\\n1.102-1 Discussion.\\n\\n1.102-2 Performance standards.\\n\\n1.102-3 Evaluating agency acquisition processes.\\n\\n1.102-4 Acquisition Team.\\n\\n1.102-5 Role of the Acquisition Team.\\n\\nSubpart 1.1 - Purpose, Authority, Issuance', metadata={'source': 'data/html/1.102.html'}),\n",
       "  0.5129168796608148),\n",
       " (Document(page_content='1.102-4 \\n         Acquisition Team.\\n\\nThe purpose of defining the Federal Acquisition Team (Team) in the Guiding Principles is to ensure that participants in the System are identified beginning with the customer and ending with the contractor of the product or service. By identifying the team members in this manner, teamwork, unity of purpose, and open communication among the members of the Team in sharing the vision and achieving the goal of the System are encouraged. Individual team members will participate in the acquisition process at the appropriate time.\\n\\n1.102 Statement of guiding principles for the Federal Acquisition System.', metadata={'source': 'data/html/1.102-4.html'}),\n",
       "  0.47294906536478243)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query vector db\n",
    "query = \"What is the purpose of the Federal Acquisition Regulations?\"\n",
    "matching_docs = db.similarity_search_with_relevance_scores(\n",
    "    query=query, \n",
    "    k=4, # number of docs to return\n",
    "    #score_threshold=.5,\n",
    "    #filter=[{\"\":\"\"}]\n",
    "    )\n",
    "\n",
    "matching_docs"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query data from your database based on your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
<<<<<<< HEAD
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
=======
    "DATA_PATH = \"data/html\"\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\" \n",
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create CLI.\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"query_text\", type=str, help=\"The query text.\")\n",
    "    args = parser.parse_args()\n",
    "    query_text = args.query_text\n",
    "\n",
    "    # Prepare the DB.\n",
<<<<<<< HEAD
    "    embedding_function = OpenAIEmbeddings()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
=======
    "    embedding_function = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "    db = Chroma.from_documents(documents=documents, embedding=embedding_function, persist_directory=CHROMA_PATH)\n",
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Unable to find matching results.\")\n",
    "        return\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(prompt)\n",
    "\n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Alternative from other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is so special about Llama 2?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
    "### Parse the augumented prompt into the chatmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human evaluation of RAG model\n",
    "Do we wanna add some other evaluation methods here??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"what safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"what safety measures were used in the development of llama 2?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb <br>\n",
    "https://github.com/pixegami/langchain-rag-tutorial/tree/main <br>\n",
    "https://www.youtube.com/watch?v=LhnCsygAvzY <br>\n",
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "I MADE SOME CHANGES HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "dl-tutorial-jB9oBwoe",
=======
   "display_name": "dl-tutorial-Si6gq9gB",
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.0"
=======
   "version": "3.11.6"
>>>>>>> d3f23497f31ced05fe7a6fb47b2a9dd1abfb03a9
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
