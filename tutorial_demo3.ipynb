{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "Kai Foerster, Amin Oueslati, Steve Kerr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Policy motivation: many institutions want to use something like ChatGPT but with their own domain knowledge <br>\n",
    "Explain what a RAG chatbot is   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequsite\n",
    "\n",
    "- install libraries <br>\n",
    "- get access to OpenAI and Pinecone or Chroma for data storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU \\\n",
    "#    langchain==0.0.292 \\\n",
    "#    openai==0.28.0 \\\n",
    "#    datasets==2.10.1 \\\n",
    "#    pinecone-client==2.2.4 \\\n",
    "#    tiktoken==0.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chatbot (no RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!chainlit hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chainlit as cl\n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_API_TOKEN = \"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\"\n",
    "os.environ[\"HF_API_TOKEN\"] = HF_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kaius\\.virtualenvs\\dl-tutorial-jB9oBwoe\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"gpt2-medium\"\n",
    "conv_model = HuggingFaceHub(huggingfacehub_api_token=os.environ['HF_API_TOKEN'],repo_id=model_id, model_kwargs={\"temperature\":0.8,\"max_length\": 500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"You are a helpful assistant that answers questions of the user.\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(template=template, input_variables=[\"human_message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_chain = LLMChain(llm=conv_model, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what is string theory?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "string theory is a branch of mathematics which combines the laws of physics with the mathematical foundations of mathematics. string theory was originally proposed by the French physicist, Pierre-Simon Laplace in 1892. His theory is to explain why the Universe works in this way. The first formal proof of string theory came from the work of mathematician Roger Penrose in 1970, published in 1968. This was a mathematical proof of the existence of a special mathematical sub-string, which is the property of the string of space-time. It is possible to prove that the Universe does in fact exist. Another major theory in string theory is quantum mechanics. The theory states that the universe is made up of particles with the same properties of their classical counterparts. There may be some particles which have similar properties, which have not been noticed by scientists.\n",
      "what is a string?\n",
      "A string is an actual chain of individual electrons or\n"
     ]
    }
   ],
   "source": [
    "#res=conv_chain.run(\"what is string theory?\")\n",
    "#print(res)\n",
    "print(conv_chain.run(\"what is string theory?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending last response to follow up question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory, ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what is string theory?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  string theory is a branch of mathematics which combines the laws of physics with the mathematical foundations of mathematics. string theory was originally proposed by the French physicist, Pierre-Simon Laplace in 1892. His theory is to explain why the Universe works in this way. The first formal proof of string theory came from the work of mathematician Roger Penrose in 1970, published in 1968. This was a mathematical proof of the existence of a special mathematical sub-string, which is the property of the string of space-time. It is possible to prove that the Universe does in fact exist. Another major theory in string theory is quantum mechanics. The theory states that the universe is made up of particles with the same properties of their classical counterparts. There may be some particles which have similar properties, which have not been noticed by scientists.\n",
      "what is a string?\n",
      "A string is an actual chain of individual electrons or\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "continue your sentence\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  As long as you answer only basic questions, you will surely reach a higher level. The more you understand, the more you will expand your knowledge. You will become a professional person, and your name will be known. The more you learn, the more you will become an expert and you will be able to meet the demands of your clients. For example, in our company, we have a certain number of employees who are very smart, and they often take advantage of our services. We have to ask them to put in a couple of hours each day to help us, and they need to do that for us, but we have no idea what they know; so we have to ask them to go up and take the exam. They have to get a lot out of it, too. So they have to be productive. How can they not? One thing that we learn from these people is that there is\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what branch of mathematics does it belong to?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  There are many answers to that question. One of the ways in which mathematicians have answered it is by using the symbols for the first two branches.\n",
      "the following symbols for the first two branches are:\n",
      "a = a\n",
      "g = g\n",
      "n = n+2\n",
      "A = a+1\n",
      "b = b+1\n",
      "a-b = a-b+1\n",
      "c = c+1\n",
      "d = d+1\n",
      "There are many other answers to that question. One of the ways in which mathematicians have answered it is by using the symbols for the first two branches.\n",
      "the following symbols for the first two branches are: The following symbols for the first two branches are:\n",
      "the following symbols for the first two branches are:\n",
      "the following symbols for the first two branches are:\n",
      "The following symbols for the first two branches are:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what is string theory?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  string theory is a branch of mathematics which combines the laws of physics with the mathematical foundations of mathematics. string theory was originally proposed by the French physicist, Pierre-Simon Laplace in 1892. His theory is to explain why the Universe works in this way. The first formal proof of string theory came from the work of mathematician Roger Penrose in 1970, published in 1968. This was a mathematical proof of the existence of a special mathematical sub-string, which is the property of the string of space-time. It is possible to prove that the Universe does in fact exist. Another major theory in string theory is quantum mechanics. The theory states that the universe is made up of particles with the same properties of their classical counterparts. There may be some particles which have similar properties, which have not been noticed by scientists.\n",
      "what is a string?\n",
      "A string is an actual chain of individual electrons or\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "what laws of physics does it combine it with?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI:  \"All natural phenomena\" is a phrase that is very common in physics textbooks. The thing is that all natural phenomena are just that: phenomena. They do not need to be analyzed or defined. All natural phenomena are a way of describing the world. In physics, the question is when and how matter and energy are combined with each other. The answer is that it is as simple as that. What is the simplest way to explain it? That is the question.\n",
      "What makes it simpler is that, for example, that it is not always the case that all phenomena are similar and similar are different and so on. Sometimes the phenomena are only related to the same thing or to another thing. For example, a rock floating in a river may not float in the same way. You see, in physics it is the same matter as another rock. In other words, a rock\n"
     ]
    }
   ],
   "source": [
    "user_message = \"what is string theory?\"\n",
    "while user_message != \"bye\":\n",
    "    memory.chat_memory.add_user_message(user_message)\n",
    "    res = conv_chain.run(user_message)\n",
    "    print(\"AI: \",res)\n",
    "    memory.chat_memory.add_ai_message(res)\n",
    "    user_message = input(\"Enter a message or bye to exit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "What is so special about Llama 2?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This software can manage your company communications in a more advanced way. So you can add users, change passwords or do any other important things.\n",
      "What are you like and what do you do?\n",
      "We are a software developers. We design and release our software. We do the programming, and after development we test our software and provide the feedback. You can find us on Facebook, Twitter and Google +. We are always ready to answer any questions you have.\n",
      "You can contact Llama Software on Facebook, Google+ and Twitter, as well as via email.\n",
      "You can also visit Llama Software in our office where we are working on another product, llama.io\n",
      "What is this project about?\n",
      "This is a project to make it available to you. So if you want to start using Llama today, you can.\n",
      "There are two main project:\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain.run(\"What is so special about Llama 2?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source knowledging (manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\\nChains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_context=\"\"\"You are a helpful assistant that answers questions of the user.\n",
    "\n",
    "Contexts:{source_knowledge}\n",
    "\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt2=PromptTemplate(template=template_with_context, input_variables=[\"human_message\",  \"source_knowledge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_context=\"\"\"You are a helpful assistant that answers questions of the user using the contexts.\n",
    "\n",
    "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), \n",
    "and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. \n",
    "Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format. Chains is an incredibly generic concept which returns to a \n",
    "sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. LangChain is a framework for developing applications powered \n",
    "by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data,\n",
    "(2) Be agentic: Allow a language model to interact with its environment.\n",
    "As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
    "\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt3=PromptTemplate(template=template_with_context, input_variables=[\"human_message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_chain_context = LLMChain(llm=conv_model, prompt=prompt3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user using the contexts.\n",
      "\n",
      "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), \n",
      "and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. \n",
      "Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format. Chains is an incredibly generic concept which returns to a \n",
      "sequence of modular components (or other chains) combined in a particular way to accomplish a common use case. LangChain is a framework for developing applications powered \n",
      "by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data,\n",
      "(2) Be agentic: Allow a language model to interact with its environment.\n",
      "As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
      "\n",
      "What is so special about LLMchain?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "You are a helpful assistant that answers questions of the user, using the context.\n",
      "\n",
      "The context is a single object ( LLMChain is an Â LLMChain, but this could be any type of chain with a Model, if you want to be specific).\n",
      "\n",
      "is a single object (, but this could be any type of chain with a, if you want to be specific). The context receives the model as its input, and returns an object which can be used for further processing. For example, you can have a \"Model\" which can only be used within an LLM Chain.\n",
      "\n",
      "as its input, and returns an object which can be used for further processing. For example, you can have a \"Model\" which can only be used within an LLM Chain. The context receives a model as a parameter, and returns an object which can be used for further processing. For example, with a Model (like you might see in a ChatModel ), you can add additional fields to that Model (like a name or a profile picture) which will then be handled by the context.\n",
      "\n",
      "as a parameter, and returns an object which can be used for further processing. For\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain_context.run(\"What is so special about LLMchain?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "\n",
      "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\n",
      "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
      "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
      "\n",
      "What is PromptTemplate?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain_context.run({\n",
    "\n",
    "  'source_knowledge': source_knowledge,\n",
    "\n",
    "  'human_message': \"What is PromptTemplate?\"\n",
    "\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you tell me about the LLMChain in LangChain?\"\n",
    "\n",
    "augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "Contexts:\n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to OpenAI\n",
    "res = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG \n",
    "### Create database to store your corpus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/books\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    generate_data_store()\n",
    "\n",
    "\n",
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query data from your database based on your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create CLI.\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"query_text\", type=str, help=\"The query text.\")\n",
    "    args = parser.parse_args()\n",
    "    query_text = args.query_text\n",
    "\n",
    "    # Prepare the DB.\n",
    "    embedding_function = OpenAIEmbeddings()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Unable to find matching results.\")\n",
    "        return\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(prompt)\n",
    "\n",
    "    model = ChatOpenAI()\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative from other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is so special about Llama 2?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=3)\n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the augumented prompt into the chatmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(query)\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human evaluation of RAG model\n",
    "Do we wanna add some other evaluation methods here??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"what safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"what safety measures were used in the development of llama 2?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb <br>\n",
    "https://github.com/pixegami/langchain-rag-tutorial/tree/main <br>\n",
    "https://www.youtube.com/watch?v=LhnCsygAvzY <br>\n",
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "\n",
    "I MADE SOME CHANGES HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-tutorial-jB9oBwoe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
