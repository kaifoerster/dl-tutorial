{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xWlzQ2ZZAKq"
      },
      "source": [
        "# Tutorial\n",
        "\n",
        "Kai Foerster, Amin Oueslati, Steve Kerr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92dgmrOzZAKt"
      },
      "source": [
        "## Introduction\n",
        "Policy motivation: many institutions want to use something like ChatGPT but with their own domain knowledge <br>\n",
        "Explain what a RAG chatbot is   <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrweYwM_ZAKt"
      },
      "source": [
        "### Next steps\n",
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_wVDPvlZAKu"
      },
      "source": [
        "# Setup\n",
        "\n",
        "* Install dependencies\n",
        "* Configure an API key for Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foQiAw4iZAKu"
      },
      "outputs": [],
      "source": [
        "# install dependencies\n",
        "!pip install -qqq bitsandbytes==0.40.0 --progress-bar off\n",
        "!pip install -qqq torch==2.0.1 --progress-bar off\n",
        "!pip install -qqq transformers==4.30.0 --progress-bar off\n",
        "!pip install -qqq accelerate==0.21.0 --progress-bar off\n",
        "!pip install -qqq xformers==0.0.20 --progress-bar off\n",
        "!pip install -qqq einops==0.6.1 --progress-bar off\n",
        "!pip install -qqq langchain==0.0.233 --progress-bar off\n",
        "!pip install -qqq sentence_transformers --progress-bar off\n",
        "!pip install -qqq chromadb --progress-bar off"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8iBKZXaZAKv"
      },
      "source": [
        "## Building a chatbot (no RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHcEjpUnZAKw",
        "outputId": "38e933ee-f908-4e05-d333-7a865a674b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-shl43yfpralf --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "import warnings\n",
        "import gc\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain import LLMChain\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.schema import BaseOutputParser\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    StoppingCriteria,\n",
        "    StoppingCriteriaList,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "M9yIH-3DbjW3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ICpr2f0UZAKw",
        "outputId": "a4ebb1b0-e6dd-425b-fd69-66f40b8c2cbe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0040dacacbb1>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    device = f'cuda:{cuda.current_device()}' uf cuda.is_available() else 'cpu'\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' uf cuda.is_available() else 'cpu'\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quant_type = 'nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        "\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\"\n",
        ")\n",
        "model = model.eval()\n",
        "\n",
        "#tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0-cwRqYZAKx"
      },
      "outputs": [],
      "source": [
        "model_id = \"tiiuae/falcon-7b-instruct\"\n",
        "conv_model = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=os.environ['HF_API_TOKEN'],\n",
        "    repo_id=model_id,\n",
        "    model_kwargs={\"temperature\":0.8,\"max_length\": 1000}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMWV3Xp3ZAKx"
      },
      "outputs": [],
      "source": [
        "#from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "#from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "#from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR10WKNNZAKx"
      },
      "outputs": [],
      "source": [
        "#tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\", padding_side=\"left\")\n",
        "#tokenizer.pad_token = tokenizer.eos_token\n",
        "#model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
        "#pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n",
        "#hf = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtLRu4yXZAKy"
      },
      "outputs": [],
      "source": [
        "template=\"\"\"You are a helpful assistant that answers questions of the user.\n",
        "{human_message}\n",
        "\"\"\"\n",
        "\n",
        "prompt=PromptTemplate(template=template, input_variables=[\"human_message\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fmEyNlQZAKy"
      },
      "outputs": [],
      "source": [
        "conv_chain = LLMChain(llm=conv_model, prompt=prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE48BhGtZAKy"
      },
      "outputs": [],
      "source": [
        "#res=conv_chain.run(\"what is string theory?\")\n",
        "#print(res)\n",
        "print(conv_chain.run(\"I would \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjCKf8BjZAKy"
      },
      "source": [
        "### Appending last response to follow up question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KARPxgeRZAKy"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory, ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Knmr7GJCZAKz"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O17kx4VZAKz"
      },
      "outputs": [],
      "source": [
        "memory_chain = ConversationChain(llm=conv_model, memory = memory, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNf5Cz5-ZAKz"
      },
      "outputs": [],
      "source": [
        "memory_chain.run(\"What is your name\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S34OrdLXZAKz"
      },
      "outputs": [],
      "source": [
        "user_message = \"whatever is this\"\n",
        "while user_message != \"bye\":\n",
        "    user_message = input(\"You: \")\n",
        "    print(memory_chain.run(user_message))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I9XV8Y6ZAKz"
      },
      "source": [
        "### Hallucinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnjgz_d1ZAKz"
      },
      "outputs": [],
      "source": [
        "print(conv_chain.run(\"What is so special about LLMChain?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JYMOJg3ZAKz"
      },
      "source": [
        "### Source knowledging (manual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN0izCD5ZAKz"
      },
      "outputs": [],
      "source": [
        "llmchain_information = [\n",
        "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
        "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
        "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
        "]\n",
        "\n",
        "source_knowledge = \"\\n\".join(llmchain_information)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6kumvI7ZAK0"
      },
      "outputs": [],
      "source": [
        "source_knowledge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0QefHf9ZAK0"
      },
      "outputs": [],
      "source": [
        "template_with_context=\"\"\"You are a helpful assistant that answers questions of the user, using the context provided below.\n",
        "\n",
        "Contexts:{source_knowledge}\n",
        "\n",
        "{human_message}\n",
        "\"\"\"\n",
        "\n",
        "prompt2=PromptTemplate(template=template_with_context, input_variables=[\"human_message\",  \"source_knowledge\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCh94m4xZAK0"
      },
      "outputs": [],
      "source": [
        "print(prompt2.format(human_message=\"What is a LLMChain?\", source_knowledge=source_knowledge))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Noq5t4VWZAK0"
      },
      "outputs": [],
      "source": [
        "context_chain = LLMChain(llm=conv_model, prompt=prompt2, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3864l_3oZAK0"
      },
      "outputs": [],
      "source": [
        "print(context_chain.run({\n",
        "\n",
        "  'source_knowledge': source_knowledge,\n",
        "\n",
        "  'human_message': \"What is LLMChain?\"\n",
        "\n",
        "}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLs0Puq8ZAK0"
      },
      "source": [
        "## RAG\n",
        "### Create database to store your corpus on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G02FFlkvZAK0"
      },
      "outputs": [],
      "source": [
        "# load dependencies\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R21hOG5kZAK0"
      },
      "outputs": [],
      "source": [
        "# set params\n",
        "DATA_PATH = \"data/html\"\n",
        "CHROMA_PATH = \"chroma_db\"\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\" # Chroma defaults to \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "# alternative: \"BAAI/bge-small-en-v1.5\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh07QeeYZAK0"
      },
      "source": [
        "# Load Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuJL3fWVZAK1"
      },
      "outputs": [],
      "source": [
        "# load docs\n",
        "def load_docs(directory):\n",
        "  loader = DirectoryLoader(directory)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "documents = load_docs(DATA_PATH)\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzWoyIOkZAK1"
      },
      "outputs": [],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8qS3yHqZAK1"
      },
      "source": [
        "# Embed Documents & Upload to Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJagrSvlZAK1"
      },
      "outputs": [],
      "source": [
        "# define text embedding model\n",
        "embedding_func = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
        "\n",
        "# See https://huggingface.co/spaces/mteb/leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEPLBmHrZAK1"
      },
      "outputs": [],
      "source": [
        "# first, clear out current db\n",
        "if os.path.exists(CHROMA_PATH):\n",
        "    shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "# initialize Chroma db and save locally\n",
        "db = Chroma.from_documents(\n",
        "    documents=documents, embedding=embedding_func, persist_directory=CHROMA_PATH\n",
        "    )\n",
        "\n",
        "db.persist()\n",
        "\n",
        "# print message\n",
        "print(f\"Saved {len(documents)} chunks to {CHROMA_PATH}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba5UAL3ZZAK1"
      },
      "source": [
        "# Query Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-zd72_BZAK5"
      },
      "outputs": [],
      "source": [
        "# query vector db\n",
        "query = \"What is the purpose of the Federal Acquisition Regulations?\"\n",
        "matching_docs = db.similarity_search_with_relevance_scores(\n",
        "    query=query,\n",
        "    k=4, # number of docs to return\n",
        "    #score_threshold=.5,\n",
        "    #filter=[{\"\":\"\"}]\n",
        "    )\n",
        "\n",
        "matching_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srqMlYCqZAK5"
      },
      "outputs": [],
      "source": [
        "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in matching_docs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEfh9YfiZAK5"
      },
      "outputs": [],
      "source": [
        "context_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXbPH77wZAK5"
      },
      "source": [
        "### Query data from your database based on your prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCsWX8BEZAK5"
      },
      "outputs": [],
      "source": [
        "### adapted version\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question based on the above context: {question}\n",
        "\"\"\"\n",
        "\n",
        "def RAG(query_text):\n",
        "    # Search the DB.\n",
        "    results = db.similarity_search_with_relevance_scores(query_text, k=6)\n",
        "    if len(results) == 0 or results[0][1] < 0.7:\n",
        "        print(f\"Unable to find matching results.\")\n",
        "        return\n",
        "\n",
        "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
        "    if len(context_text) > 1000:\n",
        "        context_text = context_text[:1000]\n",
        "        print(\"Warning: Context exceeded 1000 characters, trimming from the end.\")\n",
        "\n",
        "    prompt_template=PromptTemplate(template=PROMPT_TEMPLATE, input_variables=[\"context\",  \"question\"])\n",
        "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
        "    #print(prompt)\n",
        "\n",
        "    chain = LLMChain(llm=conv_model, prompt=prompt_template, verbose=True)\n",
        "    response_text = chain.run({\"context\": context_text, \"question\": query})\n",
        "\n",
        "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
        "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
        "    print(formatted_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVkCvr2-ZAK6"
      },
      "source": [
        "### Parse the augumented prompt into the chatmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjoJ6BNGZAK6"
      },
      "outputs": [],
      "source": [
        "RAG(\"What does the Federal Acquisition Regulations define?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfCoHLoCZAK6"
      },
      "source": [
        "### Human evaluation of RAG model\n",
        "Do we wanna add some other evaluation methods here??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrpyQKUcZAK6"
      },
      "outputs": [],
      "source": [
        "prompt = HumanMessage(\n",
        "    content=\"what safety measures were used in the development of llama 2?\"\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCHWrIGKZAK6"
      },
      "outputs": [],
      "source": [
        "prompt = HumanMessage(\n",
        "    content=augment_prompt(\n",
        "        \"what safety measures were used in the development of llama 2?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "res = chat(messages + [prompt])\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TCKUFVMZAK6"
      },
      "source": [
        "## References\n",
        "\n",
        "https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb <br>\n",
        "https://github.com/pixegami/langchain-rag-tutorial/tree/main <br>\n",
        "https://www.youtube.com/watch?v=LhnCsygAvzY <br>\n",
        "https://www.youtube.com/watch?v=tcqEUSNCn8I <br>\n",
        "https://www.mlexpert.io/prompt-engineering/chatbot-with-local-llm-using-langchain\n",
        "\n",
        "I MADE SOME CHANGES HERE"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}