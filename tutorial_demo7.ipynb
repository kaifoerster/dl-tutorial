{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "Kai Foerster, Amin Oueslati, Steve Kerr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Policy motivation: many institutions want to use something like ChatGPT but with their own domain knowledge <br>\n",
    "Explain what a RAG chatbot is   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "\n",
    "\t- Add HF token in .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "* Install dependencies\n",
    "* Configure an API key for Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling langchain\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving langchain\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing langchain...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m1560fd\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling sentence_transformers\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving sentence_transformers\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing sentence_transformers...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m1560fd\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling chromadb\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving chromadb\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing chromadb...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m1560fd\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling unstructured\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving unstructured\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing unstructured...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m1560fd\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling chainlit\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving chainlit\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing chainlit...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m1560fd\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling python-dotenv\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving python-dotenv\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing python-dotenv...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m1560fd\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling bs4\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving bs4\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing bs4...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m1560fd\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mLoading .env environment variables...\u001b[0m\n",
      "\u001b[32mCourtesy Notice\u001b[0m: Pipenv found itself running within a virtual environment, so it will automatically use that environment, instead of creating its own for any project. You can set \u001b[1mPIPENV_IGNORE_VIRTUALENVS=1\u001b[0m to force pipenv to ignore that environment and create its own instead. You can set \u001b[1mPIPENV_VERBOSITY=-1\u001b[0m to suppress this warning.\n",
      "\u001b[1;32mInstalling tqdm\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "\u001b[?25lResolving tqdm\u001b[33m...\u001b[0m\n",
      "\u001b[2K✔ Installation Succeeded\n",
      "\u001b[2K\u001b[32m⠋\u001b[0m Installing tqdm...\n",
      "\u001b[1A\u001b[2K\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(\u001b[0m\u001b[1m1560fd\u001b[0m\u001b[1m)\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pipenv install langchain\n",
    "!pipenv install sentence_transformers\n",
    "!pipenv install chromadb\n",
    "!pipenv install unstructured\n",
    "!pipenv install chainlit\n",
    "!pipenv install python-dotenv\n",
    "!pipenv install bs4\n",
    "!pipenv install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chatbot (no RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-12 14:46:07 - Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chainlit as cl\n",
    "from langchain import HuggingFaceHub, PromptTemplate, LLMChain\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "HF_API_TOKEN = os.getenv('HF_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steve/.local/share/virtualenvs/dl-tutorial-Si6gq9gB/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/steve/.local/share/virtualenvs/dl-tutorial-Si6gq9gB/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "conv_model = HuggingFaceHub(\n",
    "    huggingfacehub_api_token=os.environ['HF_API_TOKEN'], \n",
    "    repo_id=model_id, \n",
    "    model_kwargs={\"temperature\":0.8,\"max_length\": 1000}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"You are a helpful assistant that answers questions of the user.\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(template=template, input_variables=[\"human_message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_chain = LLMChain(llm=conv_model, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "How much does a cappucino cost at Pret a Manger in Berlin Mitte?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The cost of a cappucino at Pret a Manger in Berlin Mitte varies by size, but generally ranges from 4.95 to 6.95 euros.\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain.run(\"How much does a cappucino cost at Pret a Manger in Berlin Mitte?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user.\n",
      "How much does a cappucino cost at Pret a Manger in Berlin Mitte?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The cost of a cappucino at Pret a Manger in Berlin Mitte varies by size, but generally ranges from 4.95 to 6.95 euros.\n"
     ]
    }
   ],
   "source": [
    "print(conv_chain.run(\"How much does a cappucino cost at Pret a Manger in Berlin Mitte?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source knowledging (manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]\n",
    "\n",
    "source_knowledge = \"\\n\".join(llmchain_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_with_context=\"\"\"You are a helpful assistant that answers questions of the user, using the context provided below.\n",
    "\n",
    "Contexts:{source_knowledge}\n",
    "\n",
    "{human_message}\n",
    "\"\"\"\n",
    "\n",
    "prompt2=PromptTemplate(template=template_with_context, input_variables=[\"human_message\",  \"source_knowledge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that answers questions of the user, using the context provided below.\n",
      "\n",
      "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\n",
      "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
      "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
      "\n",
      "What is a LLMChain?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt2.format(human_message=\"What is a LLMChain?\", source_knowledge=source_knowledge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_chain = LLMChain(llm=conv_model, prompt=prompt2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a helpful assistant that answers questions of the user, using the context provided below.\n",
      "\n",
      "Contexts:A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\n",
      "Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\n",
      "LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\n",
      "\n",
      "What is Langchain?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Langchain is a framework for building conversational applications powered by language models. It allows for the creation of applications that connect to data sources, as well as the environment, enabling more intelligent and contextual conversations.\n"
     ]
    }
   ],
   "source": [
    "print(context_chain.run({\n",
    "\n",
    "  'source_knowledge': source_knowledge,\n",
    "\n",
    "  'human_message': \"What is Langchain?\"\n",
    "\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG \n",
    "### Create database to store your corpus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "DATA_PATH = \"data/html\"\n",
    "CHROMA_PATH = \"chroma_db\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\" # Chroma defaults to \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# alternative: \"BAAI/bge-small-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import SoupStrainer\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import BSHTMLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3487/3487 [00:04<00:00, 712.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3487"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define Beautiful Soup key word args\n",
    "bs_kwargs = {\n",
    "    \"features\": \"html.parser\", \n",
    "    \"parse_only\": SoupStrainer(\"p\") # only include relevant text\n",
    "}\n",
    "\n",
    "# define Loader key word args\n",
    "loader_kwargs = {\n",
    "    \"open_encoding\": \"utf-8\",\n",
    "    \"bs_kwargs\": bs_kwargs\n",
    "}\n",
    "\n",
    "# define Loader\n",
    "loader = DirectoryLoader(\n",
    "    path='data/html', \n",
    "    glob=\"*.html\", \n",
    "    loader_cls=BSHTMLLoader,\n",
    "    loader_kwargs=loader_kwargs,\n",
    "    show_progress=True\n",
    "    )\n",
    "\n",
    "# load docs\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Warranties of data shall be developed and used in accordance with agency regulations.', metadata={'source': 'data/html/46.708.html', 'title': ''})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect first doc\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add source label\n",
    "import re\n",
    "\n",
    "for doc in documents:\n",
    "    doc_source = re.search(\"\\d{1,2}[.]\\d+(\\-\\d)*\", doc.metadata[\"source\"]).group() \n",
    "    doc.metadata[\"source\"] = \" \".join([\"FAR\", doc_source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3487 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3487/3487 [00:03<00:00, 1006.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# define Beautiful Soup key word args\n",
    "bs_kwargs = {\n",
    "    \"features\": \"html.parser\", \n",
    "    \"parse_only\": SoupStrainer(\"title\") # only include relevant text\n",
    "}\n",
    "\n",
    "# define Loader key word args\n",
    "loader_kwargs = {\n",
    "    \"open_encoding\": \"utf-8\",\n",
    "    \"bs_kwargs\": bs_kwargs\n",
    "}\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path='data/html', \n",
    "    glob=\"*.html\", \n",
    "    loader_cls=BSHTMLLoader,\n",
    "    loader_kwargs=loader_kwargs,\n",
    "    show_progress=True\n",
    "    )\n",
    "\n",
    "document_titles = loader.load()\n",
    "\n",
    "# Convert the metadata for the specified label into a list\n",
    "title_list = [doc.metadata[\"title\"] for doc in document_titles]\n",
    "\n",
    "# add title label\n",
    "i = 0\n",
    "for doc in documents:\n",
    "    doc.metadata[\"title\"] = title_list[i]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add FAR part label\n",
    "# import re \n",
    "\n",
    "# for doc in docs:\n",
    "#     doc_part = re.search('^(\\d{1,2})', doc.metadata['source']).group()\n",
    "#     doc.metadata[\"part\"] = \" \".join([\"FAR Part\", doc_part])\n",
    "    \n",
    "#     print(doc.metadata[\"part\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'FAR 46.708', 'title': '46.708 Warranties of data.'},\n",
       " {'source': 'FAR 9.405', 'title': '9.405 Effect of listing.'},\n",
       " {'source': 'FAR 11.106',\n",
       "  'title': '11.106 Purchase descriptions for service contracts.'},\n",
       " {'source': 'FAR 16.204', 'title': '16.204 Fixed-price incentive contracts.'},\n",
       " {'source': 'FAR 7.201', 'title': '7.201 [Reserved]'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect metadata \n",
    "doc_metadata = [doc.metadata  for doc in documents]\n",
    "doc_metadata[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed Documents & Upload to Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-11 15:32:58 - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2023-12-11 15:32:59 - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# define text embedding model\n",
    "embedding_func = SentenceTransformerEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# See https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-11 15:33:26 - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 6/6 [00:12<00:00,  2.00s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:10<00:00,  1.79s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:10<00:00,  1.77s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:12<00:00,  2.11s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:11<00:00,  1.92s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:12<00:00,  2.03s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:10<00:00,  1.83s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:10<00:00,  1.83s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:11<00:00,  1.88s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:11<00:00,  1.84s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:10<00:00,  1.71s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:11<00:00,  1.91s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:11<00:00,  1.87s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:12<00:00,  2.01s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:12<00:00,  2.15s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:12<00:00,  2.15s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:12<00:00,  2.15s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:13<00:00,  2.18s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:13<00:00,  2.21s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:10<00:00,  1.71s/it]\n",
      "Batches: 100%|██████████| 6/6 [00:11<00:00,  1.94s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3487 chunks to chroma_db.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# first, clear out current db\n",
    "if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "# initialize Chroma db and save locally\n",
    "db = Chroma.from_documents(\n",
    "    documents=documents, embedding=embedding_func, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "\n",
    "db.persist()\n",
    "\n",
    "# print message\n",
    "print(f\"Saved {len(documents)} chunks to {CHROMA_PATH}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='1.101 Purpose.\\n\\nThe Federal Acquisition Regulations System is established for the codification and publication of uniform policies and procedures for acquisition by all executive agencies. The Federal Acquisition Regulations System consists of the Federal Acquisition Regulation (FAR), which is the primary document, and agency acquisition regulations that implement or supplement the FAR. The FAR System does not include internal agency guidance of the type described in 1.301(a)(2).\\n\\nSubpart 1.1 - Purpose, Authority, Issuance', metadata={'source': 'data\\\\html\\\\1.101.html'}),\n",
       "  0.754029959492634),\n",
       " (Document(page_content='1.000 Scope of part.\\n\\nThis part sets forth basic policies and general information about the Federal Acquisition Regulations System including purpose, authority, applicability, issuance, arrangement, numbering, dissemination, implementation, supplementation, maintenance, administration, and deviation. subparts\\xa0 1.2,1.3, and 1.4 prescribe administrative procedures for maintaining the FAR System.\\n\\nPart 1 - Federal Acquisition Regulations System', metadata={'source': 'data\\\\html\\\\1.000.html'}),\n",
       "  0.5853860254517804),\n",
       " (Document(page_content='1.102 Statement of guiding\\nprinciples for the Federal Acquisition System.\\n\\n(a) The\\nvision for the Federal Acquisition System is to deliver on a timely\\nbasis the best value product or service to the customer, while maintaining\\nthe public’s trust and fulfilling public policy objectives. Participants\\nin the acquisition process should work together as a team and should\\nbe empowered to make decisions within their area of responsibility.\\n\\n(b) The Federal\\nAcquisition System will-\\n\\n(1) Satisfy the customer in terms\\nof cost, quality, and timeliness of the delivered product or service\\nby, for example-\\n\\n(i) Maximizing the use of commercial products and\\ncommercial services;\\n\\n(ii) Using contractors\\nwho have a track record of successful past performance or who demonstrate\\na current superior ability to perform; and\\n\\n(iii) Promoting\\ncompetition;\\n\\n(2) Minimize\\nadministrative operating costs;\\n\\n(3) Conduct business\\nwith integrity, fairness, and openness; and\\n\\n(4) Fulfill public\\npolicy objectives.\\n\\n(c) The Acquisition\\nTeam consists of all participants in Government acquisition including\\nnot only representatives of the technical, supply, and procurement\\ncommunities but also the customers they serve, and the contractors\\nwho provide the products and services.\\n\\n(d) The role\\nof each member of the Acquisition Team is to exercise personal initiative\\nand sound business judgment in providing the best value product\\nor service to meet the customer’s needs. In exercising initiative,\\nGovernment members of the Acquisition Team may assume if a specific\\nstrategy, practice, policy or procedure is in the best interests\\nof the Government and is not addressed in the FAR, nor prohibited\\nby law (statute or case law), Executive order or other regulation, that\\nthe strategy, practice, policy or procedure is a permissible exercise\\nof authority.\\n\\n1.102-1 Discussion.\\n\\n1.102-2 Performance standards.\\n\\n1.102-3 Evaluating agency acquisition processes.\\n\\n1.102-4 Acquisition Team.\\n\\n1.102-5 Role of the Acquisition Team.\\n\\nSubpart 1.1 - Purpose, Authority, Issuance', metadata={'source': 'data\\\\html\\\\1.102.html'}),\n",
       "  0.5129167532202692),\n",
       " (Document(page_content='13.002 Purpose.\\n\\nThe purpose of this part is to prescribe simplified acquisition procedures in order to-\\n\\n(a)\\r\\n                  \\r\\n\\t\\t\\t\\t\\t\\t            Reduce administrative costs;\\n\\n(b)\\r\\n                  \\r\\n\\t\\t\\t\\t\\t\\t            Improve opportunities for small, small disadvantaged, women-owned, veteran-owned, HUBZone, and service-disabled veteran-owned small business concerns to obtain a fair proportion of Government contracts;\\n\\n(c)\\r\\n                  \\r\\n\\t\\t\\t\\t\\t\\t            Promote efficiency and economy in contracting; and\\n\\n(d)\\r\\n                  \\r\\n\\t\\t\\t\\t\\t\\t            Avoid unnecessary burdens for agencies and contractors.\\n\\nPart 13 - Simplified Acquisition Procedures', metadata={'source': 'data\\\\html\\\\13.002.html'}),\n",
       "  0.5057721459012481)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# query vector db\n",
    "query = \"What is the purpose of the Federal Acquisition Regulations?\"\n",
    "matching_docs = db.similarity_search_with_relevance_scores(\n",
    "    query=query, \n",
    "    k=4, # number of docs to return\n",
    "    #score_threshold=.5,\n",
    "    #filter=[{\"\":\"\"}]\n",
    "    )\n",
    "\n",
    "matching_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query data from your database based on your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### adapted version \n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "def RAG(query_text):\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=6)\n",
    "    if len(results) == 0 or results[0][1] < 0.7:\n",
    "        print(f\"Unable to find matching results.\")\n",
    "        return\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    if len(context_text) > 1000:\n",
    "        context_text = context_text[:1000]\n",
    "        print(\"Warning: Context exceeded 1000 characters, trimming from the end.\")\n",
    "\n",
    "    prompt_template=PromptTemplate(template=PROMPT_TEMPLATE, input_variables=[\"context\",  \"question\"])\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    #print(prompt)\n",
    "\n",
    "    chain = LLMChain(llm=conv_model, prompt=prompt_template, verbose=True)\n",
    "    response_text = chain.run({\"context\": context_text, \"question\": query})\n",
    "    \n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the augumented prompt into the chatmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 76.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Context exceeded 1000 characters, trimming from the end.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Answer the question based only on the following context:\n",
      "\n",
      "1.101 Purpose.\n",
      "\n",
      "The Federal Acquisition Regulations System is established for the codification and publication of uniform policies and procedures for acquisition by all executive agencies. The Federal Acquisition Regulations System consists of the Federal Acquisition Regulation (FAR), which is the primary document, and agency acquisition regulations that implement or supplement the FAR. The FAR System does not include internal agency guidance of the type described in 1.301(a)(2).\n",
      "\n",
      "Subpart 1.1 - Purpose, Authority, Issuance\n",
      "\n",
      "---\n",
      "\n",
      "1.000 Scope of part.\n",
      "\n",
      "This part sets forth basic policies and general information about the Federal Acquisition Regulations System including purpose, authority, applicability, issuance, arrangement, numbering, dissemination, implementation, supplementation, maintenance, administration, and deviation. subparts  1.2,1.3, and 1.4 prescribe administrative procedures for maintaining the FAR System.\n",
      "\n",
      "Part 1 - Federal Acquisition Regulations System\n",
      "\n",
      "---\n",
      "\n",
      "1.302 Limitat\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is the purpose of the Federal Acquisition Regulations?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: \n",
      "The purpose of the Federal Acquisition Regulations is to establish a common system for all executive agencies for implementing the Federal acquisition process, which includes policies and procedures for procurement and contracting.\n",
      "Sources: ['data\\\\html\\\\1.101.html', 'data\\\\html\\\\1.000.html', 'data\\\\html\\\\1.302.html', 'data\\\\html\\\\1.102.html', 'data\\\\html\\\\1.304.html', 'data\\\\html\\\\2.101.html']\n"
     ]
    }
   ],
   "source": [
    "RAG(\"What does the Federal Acquisition Regulations define?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human evaluation of RAG model\n",
    "Do we wanna add some other evaluation methods here??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=\"what safety measures were used in the development of llama 2?\"\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"what safety measures were used in the development of llama 2?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/rag-chatbot.ipynb <br>\n",
    "https://github.com/pixegami/langchain-rag-tutorial/tree/main <br>\n",
    "https://www.youtube.com/watch?v=LhnCsygAvzY <br>\n",
    "https://www.youtube.com/watch?v=tcqEUSNCn8I <br>\n",
    "https://towardsdatascience.com/a-3-step-approach-to-evaluate-a-retrieval-augmented-generation-rag-5acf2aba86de <br>\n",
    "https://github.com/curiousily/Get-Things-Done-with-Prompt-Engineering-and-LangChain <br>\n",
    "https://www.mlexpert.io/prompt-engineering/chatbot-with-local-llm-using-langchain <br>\n",
    "https://www.youtube.com/watch?v=N7dGOUwufBM <br>\n",
    "https://www.youtube.com/watch?v=ypzmPwLH_Q4&list=PLIUOU7oqGTLjAwPzyCu6m0wxLOlhJg8N5&index=2<br>\n",
    "https://www.youtube.com/watch?v=qMIM7dECAkc <br>\n",
    "https://www.youtube.com/watch?v=ukj_ITJKBwE&list=PLIUOU7oqGTLjAwPzyCu6m0wxLOlhJg8N5&index=6\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-tutorial-Si6gq9gB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
